{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f656aa3e",
   "metadata": {},
   "source": [
    "# Credit Balance Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44dd54c",
   "metadata": {},
   "source": [
    "_Motivated by_: https://github.com/Mashimo/datascience/blob/master/01-Regression/Regularisation.ipynb\n",
    "\n",
    "The Credit data set records Balance (average credit card debt for a number of individuals) as well as several quantitative predictors: age, cards (number of credit cards), education (years of education), income (in thousands of dollars), limit (credit limit), and rating (credit rating). In addition to these quantitative variables, we also have four qualitative variables: Gender, Student (student status), Married (marital status), and Ethnicity (Caucasian, African American or Asian).\n",
    "\n",
    "We will need pandas as before. This time, we will use [scikit-learn](http://scikit-learn.org/stable/) directly to peform the linear regression, rather than `statsmodel`, which uses libraries such as `scikit-learn` in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17687a84",
   "metadata": {
    "tags": [
     "importRead"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import os\n",
    "if not os.path.exists('res'):\n",
    "    os.makedirs('res')\n",
    "\n",
    "dataFile = \"data/Credit.csv\"\n",
    "data = pd.read_csv(dataFile, index_col=0) \n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b510952",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd266fb6",
   "metadata": {
    "tags": [
     "dataHead"
    ]
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724c0eed",
   "metadata": {
    "tags": [
     "data|Desc"
    ]
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d4fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7941c33b",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546da328",
   "metadata": {},
   "source": [
    "The `linear_model` prefers to see a separate `X` matrix of predictors and `y` vector of dependent variable. Therefore, we take the dataframe `data` and split it accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b705f52",
   "metadata": {
    "tags": [
     "sepXy"
    ]
   },
   "outputs": [],
   "source": [
    "y = data['Balance'].copy() # this is the Credit Balance vector - our 'y' vector\n",
    "X = data.copy()\n",
    "X.drop(['Balance'], axis=1, inplace=True) # X is a copy of the dataframe but with the 'Balance' vector removed, leaving just the matrix of predictors `X`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214ab066",
   "metadata": {},
   "source": [
    "### Recoding the `Gender` variable to _indicator_ form\n",
    "As presented in the data, the `Gender` variable is categorical (nominal) with two values `Male` and `Female`. Such variables are not suitable predictors for linear regression unless they are recoded as _indicator_ variables, with values either `0` or `1`.\n",
    "\n",
    "Arbitrarily, we can map ` Male` to `0` and `Female` to `1`. According to this mapping, the `Gender` variable could be interpreted as `IsFemale`. However, we could swap the mappings without changing the structure of the model, although the interpretation of the `Gender` coefficient would change to `IsMale`.\n",
    "\n",
    "Note the space character before \"Male\" in the mapping statement below as this is present in the data, and the strings need to match exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467bdaa0",
   "metadata": {
    "tags": [
     "recodeGender"
    ]
   },
   "outputs": [],
   "source": [
    "X.Gender = X.Gender.map({'Female':1, ' Male':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493a7569",
   "metadata": {},
   "source": [
    "### Recoding the `Student` and `Married` variables in indicator form\n",
    "\n",
    "These variables are more obviously binary-values, so they can be mapped using the same approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6bb5dc",
   "metadata": {
    "tags": [
     "recodeStudentMarried"
    ]
   },
   "outputs": [],
   "source": [
    "X.Student = X.Student.map({'Yes':1, 'No':0})\n",
    "X.Married = X.Married.map({'Yes':1, 'No':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4231bf",
   "metadata": {},
   "source": [
    "### Recoding the 3-valued `Ethnicity` variable as _three_ indicator variables\n",
    "The `Ethnicity` variable has 3 values and so cannot be mapped to a single (1,1)-valued variable. Pandas provides a utility function to create indicator variables for each of those three values. These variables are added to the `X` matrix and the redundant `Ethnicity` variable should be removed from `X`, as its information is captured in the 3 new indicator variables.\n",
    "\n",
    "Optionally, these 3 indicator variables can be combined into two indicator variables. For example, let the `Asian` variable be defined implictly as the case wehn both `Caucasian` and `AfricanAmerican` take the value `0`. From a modelling perspective, this is fine. However, this can make interpretation slightly more difficult, so we choose not to combine them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6145ac",
   "metadata": {
    "tags": [
     "recodeEthnicity"
    ]
   },
   "outputs": [],
   "source": [
    "ethnicityIndicators = pd.get_dummies(X['Ethnicity'])\n",
    "X = X.join(ethnicityIndicators)\n",
    "X.drop(['Ethnicity'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20c094",
   "metadata": {},
   "source": [
    "## Single predictor models\n",
    "With `scikit-learn`, we first create a template model, which we later configure for specific fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bda196",
   "metadata": {
    "tags": [
     "linearModel"
    ]
   },
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3367d824",
   "metadata": {},
   "source": [
    "### Credit Balance by `Gender`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6a0f96",
   "metadata": {},
   "source": [
    "With `scikit-learn`, there is no model formula as there was with `statsmodels`. Instead you need to use columns in the dataframe. Also note that the constant term is assumed (so there is no need to `add_constant`.\n",
    "\n",
    "`sklearn` objects come with a `fit` function. In this case, you need to provide $X$ (the dataframe of features) and $y$ the values to fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8ba892",
   "metadata": {
    "tags": [
     "lmGender"
    ]
   },
   "outputs": [],
   "source": [
    "model.fit(X[['Gender']], y)\n",
    "[model.intercept_, model.coef_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d01ae",
   "metadata": {},
   "source": [
    "We can interpret this as follows. The average credit balance is \\$509.80\\$ for males and \\$509.80 + \\$19.73 = \\$529.53\\$ for females.\n",
    "\n",
    "However this is probably a gross simplification, as we are ignoring all the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76890d61",
   "metadata": {},
   "source": [
    "### Exercise: How does the Credit Balance vary with each of the other categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c5ba4d",
   "metadata": {},
   "source": [
    "## Forward selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e6b455",
   "metadata": {},
   "source": [
    "Previously, in the case of the Diamonds and Advertising data, we used forward selection to find the best set of predictors. We used a manual search that worked well because the number of candidate predictors was quite low.\n",
    "\n",
    "For a larger set, as here, it makes sense to leave much of the work to a python program.\n",
    "\n",
    "We identify two operations:\n",
    "\n",
    "1. Finding the $R^2$ metric for each of a candidate set of predictors, and returning the predictor with the largest metric\n",
    "2. Accumulate the predictors found to date, and look for the next best predictor.\n",
    "\n",
    "The following Python function `findNextBestPredictor()` provides operation 1., and the next block of python provides operation 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22d9b3",
   "metadata": {
    "tags": [
     "findNextBestPred"
    ]
   },
   "outputs": [],
   "source": [
    "def findNextBestPredictor(X,foundPredictors):\n",
    "    nP = X.shape[1] # number of columns in X\n",
    "    allPredictors = list(X) # See https://stackoverflow.com/a/19483025\n",
    "    predictorsToSearch = set(allPredictors) - set(foundPredictors)\n",
    "    maxScore = 0 # can usually do better than this!\n",
    "    for predictor in predictorsToSearch: # loop over all remaining columns (predictors) in X\n",
    "        trialPredictors = set(foundPredictors)\n",
    "        trialPredictors.add(predictor) # Add this predictor to the existing predictors\n",
    "        XcolSubset = X.loc[:,trialPredictors] # all rows and just the trial predictors\n",
    "        model.fit(XcolSubset, y) # fit the model to y\n",
    "        score = model.score(XcolSubset, y)\n",
    "        if score > maxScore: # identify the largest score and its associated predictor\n",
    "            maxScore = score\n",
    "            bestPredictorFound = predictor\n",
    "\n",
    "    return (maxScore, bestPredictorFound)\n",
    "\n",
    "findNextBestPredictor(X, list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b396ec",
   "metadata": {
    "tags": [
     "foundPredictors"
    ]
   },
   "outputs": [],
   "source": [
    "nP = X.shape[1]\n",
    "scores = [0]\n",
    "foundPredictors = list()\n",
    "\n",
    "for i in range(nP): # loop over all columns (predictors) in X\n",
    "    (score, bestPredictorFound) = findNextBestPredictor(X, foundPredictors)\n",
    "    foundPredictors.append(bestPredictorFound)\n",
    "    scores.append(score)\n",
    "\n",
    "print(foundPredictors)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c746aa",
   "metadata": {},
   "source": [
    "The first features are Rating, Income and Student.  With these 3 features, the $R^2$ score is already at 0.95 and it increases only marginally thereafter, see plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd9293",
   "metadata": {
    "tags": [
     "rSqByPred"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Score versus predictors\")\n",
    "plt.xlabel('Number of predictors') \n",
    "plt.ylabel('R squared')\n",
    "plt.plot(scores)\n",
    "resFig = \"res/rSqByPred.pdf\"\n",
    "plt.savefig(resFig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c15c5bd",
   "metadata": {},
   "source": [
    "The first 3 features are the best predictors, the others do not add much value. As we add more and more parameters to our model its complexity increases. Generally, this results in increasing variance and decreasing bias, i.e. overfitting, so the model does not generalise as well from the training data to new data. We have two options:\n",
    "\n",
    "1. Use model selection techniques, e.g., use `Rating`, `Income` and `Student` as predictors, and ignore the remaining predictors as offering little explanatory power. We have described this approach before, in other notebooks.\n",
    "2. Use regularisation techniques: _ridge regression_ and the _lasso_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6199175",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "If we attempt to use too many predictors, we can run into multicollinearity issues, where several predictors compete to explain the data. The regression procedure will choose one of the set, and the remaining predictors in that multicollinear set have extremely high variance.\n",
    "This is not good news. We want the set of predictors to have roughly equal, small variance.\n",
    "\n",
    "If you imagine the function we wish to minimise (the sum of the squares of the residuals) as a function of the $\\beta$ parameters, it will look like an inverted ridge, with more variance along the spine of the ridge than across it. We wish to constrain the OLS search to home in on the \"best\" point, and not to get distracted by running off along the ridge.\n",
    "\n",
    "This is the motivation for both ridge and lasso regression.\n",
    "\n",
    "### Standardised scaling\n",
    "\n",
    "Before we compare $\\beta$ for different types of regression, we should note that the linear model looks after scaling in a natural way. Recall that $\\hat{y}_i = \\sum_j X(i,j) \\beta_j $ can be written as $ \\hat{y}_i = \\sum_j (cX(i,j)) \\frac{\\beta_j}{c}$ for any nonzero $c$. Therefore, if we wish to ensure that the Euclidean norm of each column of $X$ is 1, or each column has the same variance, we can scale the $X$ matrix appropriately.\n",
    "\n",
    "In that regard, `scikit-learn` provides a `StandardScaler()` to derive $c$ and `scaler.fit_transform()` to apply this scaling to $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72fff28",
   "metadata": {
    "tags": [
     "stdScaler"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaledXarray = scaler.fit_transform(X) # Note that X is no longer a dataframe after scaling :-(\n",
    "scaledX = pd.DataFrame(scaledXarray, index=X.index, columns=X.columns) # Convert it back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd66a03",
   "metadata": {},
   "source": [
    "We first have a look at the size of the regression coefficients $\\beta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731b635",
   "metadata": {
    "tags": [
     "coef"
    ]
   },
   "outputs": [],
   "source": [
    "model.fit(scaledX, y)\n",
    "predictors = scaledX.columns\n",
    "coef = pd.Series(model.coef_,predictors).sort_values()\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3feba",
   "metadata": {},
   "source": [
    "Note that there is a huge difference in the sensitivity of the regression across all the predictors, which is something we often see when there are too many predictors. `scikit-learn` helpfully offers a plot of the coefficients $\\beta$ that shows this even more clearly, see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d315a716",
   "metadata": {
    "tags": [
     "plotCoef"
    ]
   },
   "outputs": [],
   "source": [
    "fig = coef.plot(kind='bar', title=\"Model coefficients\")\n",
    "resFig = \"res/coeffs.pdf\"\n",
    "plt.savefig(resFig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce0eeaf",
   "metadata": {},
   "source": [
    "Interestingly, the equivalent plot for unscaled data showed that the $|\\beta_{\\rm Cards}|$ coefficient was much larger than the others and so it dominates the regression model. Yet we also know that, on its own, it adds little to the regression. Therefore it was trying to compensate for other predictors, rather than adding much of value in its own right. The scaling helped to reduce this problem but there are still significant differences in the size of different coefficients.\n",
    "\n",
    "### Ridge Regression\n",
    "\n",
    "The ridge regression coefficients $\\beta_{\\rm (ridge)}$ minimize\n",
    "\n",
    "$\\sum_i (y_i - X_i\\beta_{\\rm (ridge)})^T(y_i - X_i\\beta_{\\rm (ridge)}) + \\lambda ||\\beta_{\\rm (ridge)}||_2$\n",
    "\n",
    "The first term is the ordinary least squares (OLS) estimate. The second term is a measure of the size of the $\\beta$ coefficients. If we minimise both together, we are looking for the $\\beta$ coefficients that (simultaneously) make the regression fit close to the data _and_ have small values (measured as their Euclidean distance from the origin in $\\beta$-space).\n",
    "\n",
    "The $\\lambda$ multiplier weights the penalty term (the one that shrinks the coefficients) relative to the normal OLS term. Small values of $\\lambda$ favour the OLS objective; larger ones favour the shrinkage objective. The easiest way to find a \"good\" $\\lambda$ and hence a good overall fit is by cross validation.\n",
    "\n",
    "![Ridge Regression: geometrical interpretation](pic/ridgeRegression.png)\n",
    "\n",
    "In the visualisation above, the OLS component of the objective function has the characteristic contours of a ridge with its minimum indicated with a red dot. The regularisation component is represented by the disk, with its minimum represented by the red dot at the origin. The overall objective depends on the scaling of the disk and where it intersects with the ridge, represented by the third red dot above. As can be seen,\n",
    "\n",
    "* the ridge estimate does not usually coincide with the OLS estimate, and\n",
    "* it generally has as many dimensions (2 nonzero values in this case) as the OLS estimate.\n",
    "\n",
    "Note that the combined objective function used in ridge regression is _not_ scale-invariant. Hence $X$ should be scaled appropriately so that different fits can be compared across $\\lambda$, without the $\\lambda$ values themselves affecting the scaling of the $\\beta$ coefficients.\n",
    "\n",
    "Here is an example using `scikit-learn`'s `Ridge()` function. For this example, we set $\\lambda$ (specified as `alpha` in the call to `Ridge`) to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0e7d7",
   "metadata": {
    "tags": [
     "ridgeFit"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# assign lambda (or alpha as is used in Ridge())\n",
    "ridgeReg = Ridge(alpha=50)\n",
    "\n",
    "# Now compute the ridge regression using X, the full matrix of predictors, and y\n",
    "ridgeReg.fit(scaledX, y)\n",
    "\n",
    "ridgeReg.score(scaledX, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0228d7e5",
   "metadata": {
    "tags": [
     "ridgeCoef"
    ]
   },
   "outputs": [],
   "source": [
    "coef = pd.Series(ridgeReg.coef_,predictors).sort_values()\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb47245",
   "metadata": {},
   "source": [
    "As you can see, the $R^2$ score is reduced, reflecting the fact that the OLS criterion is no longer the only criterion and so the bias is greater, but it is also clear that the size of the coefficients has also shrunk and the predictors with the largest $|\\beta|$ tend to be those we know from previous analysis are the more important predictors anyway.\n",
    "\n",
    "### How does the penalty weight $\\lambda$ affect the coefficients $\\mathbf{\\beta}$?\n",
    "\n",
    "We can collect the coefficients $\\beta$ for a selection of $\\lambda$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9729b6",
   "metadata": {
    "tags": [
     "coefsForLambda"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lambdas = [0.01, 1, 100, 10000]\n",
    "coefficients = np.zeros(shape=(nP, len(lambdas)))\n",
    "i=0\n",
    "for l in lambdas:\n",
    "    ridgeReg = Ridge(alpha=l)\n",
    "    ridgeReg.fit(scaledX, y)\n",
    "    coefficients[:,i] = ridgeReg.coef_\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350f12fa",
   "metadata": {},
   "source": [
    "Now we can plot the coefficients as $\\lambda$ increases, highlighting those of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.title(\"Ridge regularisation\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"standardised coefficients\")\n",
    "styles=['-','--','-.',':']\n",
    "\n",
    "labels = ['0.01','','1','', '100','', '1e4','']\n",
    "ax.set_xticklabels(labels) # custom x labels\n",
    "\n",
    "chosenPredictors = {\"Income\", \"Rating\", \"Student\"}\n",
    "for i in range(0,nP):\n",
    "    s = styles[i % len(styles)]\n",
    "    if predictors[i] in chosenPredictors:\n",
    "        plt.plot(coefficients[i], label=predictors[i], linestyle=s)\n",
    "    else:\n",
    "        plt.plot(coefficients[i])\n",
    "\n",
    "plt.legend(loc='best')\n",
    "resFig = \"res/ridge.pdf\"\n",
    "plt.savefig(resFig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39ea231",
   "metadata": {},
   "source": [
    "Each curve corresponds to the ridge regression coefficient estimate for one of the variables, plotted as a function of $\\lambda$.\n",
    "\n",
    "At the left side of the plot, $\\lambda=0.01$ is essentially zero, and so the corresponding ridge coefficient estimates are the same as the usual least squares estimates. But as $\\lambda$ increases, the ridge coefficient estimates shrink towards zero. When $\\lambda$ is extremely large, all of the ridge coefficient estimates are basically zero; this corresponds to the null regression model that contains no predictors and just the regularisation component of the objective function.\n",
    "\n",
    "In this plot, the income, rating, and student variables are highlighted, since these variables are the ones to have the largest coefficient estimates. The orange trace is interesting - it appears to represent the intercept term $\\beta_0$.\n",
    "\n",
    "As noted in the visualisation of ridge regression earlier, ridge regression generally does not reduce the number of active predictors - it just drives many of them near to zero as $\\lambda$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a3e3ee",
   "metadata": {},
   "source": [
    "## The lasso\n",
    "\n",
    "The lasso regression coefficients $\\beta_{\\rm (lasso)}$ minimize\n",
    "\n",
    "$\\sum_i (y_i - X_i\\beta_{\\rm (lasso)})^T(y_i - X_i\\beta_{\\rm (lasso)}) + \\lambda ||\\beta_{\\rm (lasso)}||_1$\n",
    "\n",
    "The first term is the ordinary least squares (OLS) estimate. The second term is a measure of the size of the $\\beta$ coefficients. Here the Manhattan norm is used, where the Euclidean norm was used for ridge regression. If we minimise both together, we are looking for the $\\beta$ coefficients that (simultaneously) make the regression fit close to the data _and_ have small values (measured as their Manhattan distance from the origin in $\\beta$-space).\n",
    "\n",
    "![Lasso Regression: geometrical interpretation](pic/lassoRegression.png)\n",
    "\n",
    "In the visualisation above, the OLS component of the objective function has the characteristic contours of a ridge with its minimum indicated with a red dot. The regularisation component is represented by the rotated square, with its minimum represented by the red dot at the origin. The overall objective depends on the scaling of the square and where it intersects with the ridge, represented by the third red dot above. As can be seen,\n",
    "\n",
    "* the lasso estimate does not usually coincide with the OLS estimate, and\n",
    "* it generally has fewer dimensions (1 in this case, since $\\beta_0 = 0$) as the OLS estimate (which has 2 in this case), because the intersection point is generally at a corner of the hypercube (square in this case) bounding the penalty component of the objective function.\n",
    "\n",
    "Using lasso regression from `sklearn` is very similar to how ridge regression is used. Again, we apply lasso regression to a range of $\\lambda$ values and review how the $\\beta$ coefficients tend to 0 as $\\lambda$ increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812341f9",
   "metadata": {
    "tags": [
     "lassoFit"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lambdas = [0.01, 1, 10, 50, 100, 200, 500, 1000]\n",
    "coefficients = np.zeros(shape=(nP, len(lambdas)))\n",
    "i=0\n",
    "for l in lambdas:\n",
    "    lassoReg = Lasso(alpha=l)\n",
    "    lassoReg.fit(scaledX, y)\n",
    "    coefficients[:,i] = lassoReg.coef_\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62235dc",
   "metadata": {
    "tags": [
     "coefsForLambdaLasso"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.title(\"Lasso Regularisation\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"standardised coefficients\")\n",
    "styles=['-','--','-.',':']\n",
    "\n",
    "labels = ['0.01','1','10','50', '100', '200','500','1000']\n",
    "\n",
    "# See https://stackoverflow.com/a/63755285/1988855\n",
    "\n",
    "# Outdated code, generates warning but works\n",
    "ax.set_xticklabels(labels) # custom x labels\n",
    "\n",
    "chosenPredictors = {\"Income\", \"Rating\", \"Student\"}\n",
    "for i in range(0,12):\n",
    "    s = styles[i % len(styles)]\n",
    "    if predictors[i] in chosenPredictors:\n",
    "        plt.plot(coefficients[i], label=predictors[i], linestyle=s)\n",
    "    else:\n",
    "        plt.plot(coefficients[i])\n",
    "\n",
    "plt.legend(loc='best')\n",
    "resFig = \"res/lasso.pdf\"\n",
    "plt.savefig(resFig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b16be21",
   "metadata": {},
   "source": [
    "$\\lambda$ has a similar job in both ridge and lasso regression, but the behaviour is different. Look at how the optimum `Rating` actually _increases_ over the range $10 < \\lambda < 500$ while all the other $\\beta$ coefficients were driven to zero.\n",
    "\n",
    "The challenge is to choose this tuning parameter ($\\lambda$).\n",
    "\n",
    "## Selecting the Tuning Parameter lambda\n",
    "\n",
    "Cross-validation provides a simple way to tackle this problem.\n",
    "\n",
    "1. Choose a grid of $\\lambda$ values\n",
    "2. Compute the cross-validation error for each value of $\\lambda$.\n",
    "3. Select the tuning parameter $\\lambda$ for which the cross-validation error is smallest.\n",
    "4. Re-fit using all the observations and the selected value of $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a74bee",
   "metadata": {},
   "source": [
    "Cross-Validation with $n=10$ splits is relatively straightforward to implement, although it requires significant computation. The data is randomly partitioned into $n$ subsets. $n-1$ of these subsets are used for training, and the remaining subset is used for validation. The prediction error (root-mean-square residual error) is computed for this validation subset. The procedure is repeated $n=10$ times in all, so that each of the $n$ subsets has a turn as the validation set. The average prediction error is computed and returned as the CV score. The python code below shows how this can be achieved using `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc2572c",
   "metadata": {
    "tags": [
     "oreCrossVaql"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "lambdas = np.linspace(500,0.01,num=50) # Step 1\n",
    "scoresCV = []\n",
    "for l in lambdas:\n",
    "    lassoReg = Lasso(alpha=l)\n",
    "    lassoReg.fit(scaledX, y)    \n",
    "    \n",
    "    scoreCV = cross_val_score(lassoReg, scaledX, y, scoring='neg_mean_squared_error', \n",
    "                             cv=KFold(n_splits=10, shuffle=True,\n",
    "                                            random_state=1))\n",
    "    scoresCV.append(np.mean(scoreCV))\n",
    "# Step 2 complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc951db",
   "metadata": {
    "tags": [
     "lambdaLassoCV"
    ]
   },
   "outputs": [],
   "source": [
    "plt.title(\"Lambda tuning for Lasso Regularisation\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"Cross-Validation error (MSE)\")\n",
    "\n",
    "plt.plot(lambdas,scoresCV)\n",
    "resFig = \"res/lambdaLassoCV.pdf\"\n",
    "plt.savefig(resFig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a48ec6",
   "metadata": {},
   "source": [
    "The plot shows how the CV error changes with $\\lambda$. It looks like the CV error is minimised near $\\lambda = 0$.\n",
    "\n",
    "Using `LassoCV`, which applies cross-validation to the Lasso, we can find the $\\lambda$ that minimises the CV error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c50da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lassoCV = LassoCV()\n",
    "lassoCV.fit(scaledX, y)\n",
    "\n",
    "lassoCV.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f332d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoReg = Lasso(lassoCV.alpha_)\n",
    "lassoReg.fit(scaledX, y)\n",
    "lassoReg.score(scaledX, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b5b479",
   "metadata": {},
   "source": [
    "## Exercise: Repeat this CV procedure to find the optimal value of $\\lambda$ and the score for _ridge_ regression instead of _lasso_ regression."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
